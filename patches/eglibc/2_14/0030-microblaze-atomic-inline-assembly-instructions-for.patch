From 9d14068885a0da7dd32b8f54c0a52873254d88cd Mon Sep 17 00:00:00 2001
From: David Holsgrove <david.holsgrove@petalogix.com>
Date: Mon, 7 May 2012 23:18:38 +1000
Subject: [PATCH 30/41] microblaze: atomic inline assembly instructions for;

 * atomic_compare_and_exchange_val_acq
 * atomic_compare_and_exchange_val_rel
 * atomic_exchange_acq
 * atomic_exchange_rel
 * atomic_exchange_and_add
 * atomic_increment_val
 * atomic_decrement_val

Signed-off-by: David Holsgrove <david.holsgrove@petalogix.com>
---
 sysdeps/microblaze/bits/atomic.h |  262 ++++++++++++++++++++++++++++++++++----
 1 files changed, 235 insertions(+), 27 deletions(-)

diff --git a/sysdeps/microblaze/bits/atomic.h b/sysdeps/microblaze/bits/atomic.h
index da0e941..91d96a6 100644
--- a/sysdeps/microblaze/bits/atomic.h
+++ b/sysdeps/microblaze/bits/atomic.h
@@ -35,34 +35,242 @@ typedef uintptr_t uatomicptr_t;
 typedef intmax_t atomic_max_t;
 typedef uintmax_t uatomic_max_t;
 
-//void __microblaze_link_error (void);
-
-/* REVISIT: This is not atomic, we need to use a swap instruction */
-#define atomic_exchange_acq(mem, newvalue)          \
-   ({ __typeof (*mem) result;                       \
-      result = *mem;                                \
-      *mem = newvalue;                              \
-      result; })
-
-/* Atomic compare and exchange.  These sequences are not actually atomic;
-   there is a race if *MEM != OLDVAL and we are preempted between the two
-   swaps.  */
-
-#define __arch_compare_and_exchange_val_8_acq(mem, newval, oldval) 	\
-   ({ __typeof (*mem) archmem = (*mem);           \
-      __typeof (oldval) archold = (oldval);       \
-      if (archmem == oldval) *mem = newval;       \
-      archold; })
-
-#define __arch_compare_and_exchange_val_16_acq(mem, newval, oldval)     \
+
+/*
+ * Microblaze does not have byte and halfword forms of load and reserve and
+ * store conditional. So for microblaze we stub out the 8- and 16-bit forms.
+ */
+#define __arch_compare_and_exchange_bool_8_acq(mem, newval, oldval)                           \
+  (abort (), 0)
+
+#define __arch_compare_and_exchange_bool_16_acq(mem, newval, oldval)                          \
+  (abort (), 0)
+
+#define __arch_compare_and_exchange_bool_8_rel(mem, newval, oldval)                           \
+  (abort (), 0)
+
+#define __arch_compare_and_exchange_bool_16_rel(mem, newval, oldval)                          \
+  (abort (), 0)
+
+#define __arch_compare_and_exchange_val_32_acq(mem, newval, oldval)                           \
+  ({                                                                                          \
+      __typeof (*(mem)) __tmp;                                                                \
+      __typeof (mem)  __memp = (mem);                                                         \
+      int test;                                                                               \
+      __asm __volatile (                                                                      \
+                "   addc    r0, r0, r0;"    /* clean carry bit*/                              \
+                "1: lwx     %0, %3, r0;"    /* atomic_add_return */                           \
+                "   addic   %1, r0, 0;"     /* check Carry*/                                  \
+                "   bnei    %1, 1b;"        /* jump if Carry is set*/                         \
+                "   cmp     %1, %0, %4;"    /* compare signed values loaded/newval */         \
+                "   bnei    %1, 2f;"        /* if is not equal - jump over */                 \
+                "   swx     %5, %3, r0;"    /* save newval */                                 \
+                "   addic   %1, r0, 0;"     /* check Carry if saving was OK */                \
+                "   bnei    %1, 1b;"        /* jump if swx wasn't successful */               \
+                "2:"                                                                          \
+                    : "=&r" (__tmp),        /* %0 */                                          \
+                    "=&r" (test),           /* %1 */                                          \
+                    "=m" (*__memp)          /* Dummy output so gcc knows writing *__memp */   \
+                    : "r" (__memp),         /* %3 */                                          \
+                    "r" (oldval),           /* %4 */                                          \
+                    "r" (newval)            /* %5 */                                          \
+                    : "cc", "memory");                                                        \
+      __tmp;                                                                                  \
+  })
+
+#define __arch_compare_and_exchange_val_64_acq(mem, newval, oldval)                           \
   (abort (), (__typeof (*mem)) 0)
 
-/* REVISIT : This is not atomic */
-#define __arch_compare_and_exchange_val_32_acq(mem, newval, oldval)	\
-   ({ __typeof (*mem) archmem = (*mem);           \
-      __typeof (oldval) archold = (oldval);       \
-      if (archmem == oldval) *mem = newval;       \
-      archold; })
+#define atomic_compare_and_exchange_val_acq(mem, newval, oldval)                              \
+  ({                                                                                          \
+    __typeof (*(mem)) __result;                                                               \
+    if (sizeof (*mem) == 4)                                                                   \
+      __result = __arch_compare_and_exchange_val_32_acq(mem, newval, oldval);                 \
+    else if (sizeof (*mem) == 8)                                                              \
+      __result = __arch_compare_and_exchange_val_64_acq(mem, newval, oldval);                 \
+    else                                                                                      \
+       abort ();                                                                              \
+    __result;                                                                                 \
+  })
+
+#define atomic_compare_and_exchange_val_rel(mem, newval, oldval)                              \
+  ({                                                                                          \
+    __typeof (*(mem)) __result;                                                               \
+    if (sizeof (*mem) == 4) {                                                                 \
+        if (__builtin_types_compatible_p (typeof (*mem), int))                                \
+            __result = __arch_compare_and_exchange_val_32_acq(mem, newval, oldval);           \
+        else if (__builtin_types_compatible_p (typeof (*mem), unsigned int))                  \
+            __result = __arch_compare_and_exchange_val_32_acq_unsigned(mem, newval, oldval);  \
+    }                                                                                         \
+    else if (sizeof (*mem) == 8)                                                              \
+      __result = __arch_compare_and_exchange_val_64_acq(mem, newval, oldval);                 \
+    else                                                                                      \
+       abort ();                                                                              \
+    __result;                                                                                 \
+  })
+
+#define __arch_atomic_exchange_32_acq(mem, value)                                             \
+  ({                                                                                          \
+      __typeof (*(mem)) __tmp;                                                                \
+      __typeof (mem)  __memp = (mem);                                                         \
+      int test;                                                                               \
+      __asm __volatile (                                                                      \
+                "   addc    r0, r0, r0;"    /* clean carry bit*/                              \
+                "1: lwx     %0, %4, r0;"    /* atomic_add_return */                           \
+                "   addic   %1, r0, 0;"     /* check Carry*/                                  \
+                "   bnei    %1, 1b;"        /* jump if Carry is set*/                         \
+                "   swx     %3, %4, r0;"    /* save newval */                                 \
+                "   addic   %1, r0, 0;"     /* check Carry if saving was OK */                \
+                "   bnei    %1, 1b;"        /* jump if swx wasn't successful */               \
+                    : "=&r" (__tmp),        /* %0 */                                          \
+                    "=&r" (test),           /* %1 */                                          \
+                    "=m" (*__memp)          /* Dummy output so gcc knows writing *__memp */   \
+                    : "r" (value),          /* %3 */                                          \
+                    "r" (__memp)            /* %4 */                                          \
+                    : "cc", "memory");                                                        \
+      __tmp;                                                                                  \
+  })
+
+#define __arch_atomic_exchange_64_acq(mem, newval)                                            \
+  (abort (), (__typeof (*mem)) 0)
+
+#define atomic_exchange_acq(mem, value)                                                       \
+  ({                                                                                          \
+    __typeof (*(mem)) __result;                                                               \
+    if (sizeof (*mem) == 4)                                                                   \
+      __result = __arch_atomic_exchange_32_acq (mem, value);                                  \
+    else if (sizeof (*mem) == 8)                                                              \
+      __result = __arch_atomic_exchange_64_acq (mem, value);                                  \
+    else                                                                                      \
+       abort ();                                                                              \
+    __result;                                                                                 \
+  })
+
+#define atomic_exchange_rel(mem, value)                                                       \
+  ({                                                                                          \
+    __typeof (*(mem)) __result;                                                               \
+    if (sizeof (*mem) == 4)                                                                   \
+      __result = __arch_atomic_exchange_32_acq (mem, value);                                  \
+    else if (sizeof (*mem) == 8)                                                              \
+      __result = __arch_atomic_exchange_64_acq (mem, value);                                  \
+    else                                                                                      \
+       abort ();                                                                              \
+    __result;                                                                                 \
+  })
+
+#define __arch_atomic_exchange_and_add_32(mem, value)                                         \
+  ({                                                                                          \
+    __typeof (*(mem)) __tmp;                                                                  \
+      __typeof (mem)  __memp = (mem);                                                         \
+    int test;                                                                                 \
+    __asm __volatile (                                                                        \
+                "   addc    r0, r0, r0;"    /* clean carry bit*/                              \
+                "1: lwx     %0, %4, r0;"    /* atomic_add_return */                           \
+                "   addic   %1, r0, 0;"     /* check Carry*/                                  \
+                "   bnei    %1, 1b;"        /* jump if Carry is set*/                         \
+                "   add     %1, %3, %0;"    /* add value */                                   \
+                "   swx     %1, %4, r0;"    /* save value back */                             \
+                "   addic   %1, r0, 0;"     /* check Carry if saving was OK */                \
+                "   bnei    %1, 1b;"        /* jump if swx wasn't successful */               \
+                    : "=&r" (__tmp),        /* %0 */                                          \
+                    "=&r" (test),           /* %1 */                                          \
+                    "=m" (*__memp)          /* Dummy output so gcc knows writing *__memp */   \
+                    : "r" (value),          /* %3 */                                          \
+                    "r" (__memp)            /* %4 */                                          \
+                    : "cc", "memory");                                                        \
+    __tmp;                                                                                    \
+  })
 
-#define __arch_compare_and_exchange_val_64_acq(mem, newval, oldval) \
+#define __arch_atomic_exchange_and_add_64(mem, value)                                         \
   (abort (), (__typeof (*mem)) 0)
+
+#define atomic_exchange_and_add(mem, value)                                                   \
+  ({                                                                                          \
+    __typeof (*(mem)) __result;                                                               \
+    if (sizeof (*mem) == 4)                                                                   \
+      __result = __arch_atomic_exchange_and_add_32 (mem, value);                              \
+    else if (sizeof (*mem) == 8)                                                              \
+      __result = __arch_atomic_exchange_and_add_64 (mem, value);                              \
+    else                                                                                      \
+       abort ();                                                                              \
+    __result;                                                                                 \
+  })
+
+#define __arch_atomic_increment_val_32(mem)                                                   \
+  ({                                                                                          \
+    __typeof (*(mem)) __val;                                                                  \
+    int test;                                                                                 \
+    __asm __volatile (                                                                        \
+                "   addc    r0, r0, r0;"    /* clean carry bit*/                              \
+                "1: lwx     %0, %3, r0;"    /* atomic_add_return*/                            \
+                "   addic   %1, r0, 0;"     /* check Carry*/                                  \
+                "   bnei    %1, 1b;"        /* jump if Carry is set*/                         \
+                "   addi    %0, %0, 1;"     /* add 1 */                                       \
+                "   swx     %0, %3, r0;"    /* save value back */                             \
+                "   addic   %1, r0, 0;"     /* check Carry if saving was OK */                \
+                "   bnei    %1, 1b;"        /* jump if swx wasn't successful */               \
+                    : "=&r" (__val),        /* %0 */                                          \
+                    "=&r" (test),           /* %1 */                                          \
+                    "=m" (*mem)             /* Dummy output so gcc knows writing *mem */      \
+                    : "r" (mem),            /* %3 */                                          \
+                    "m" (*mem)              /* %4 */                                          \
+                    : "cc", "memory");                                                        \
+    __val;                                                                                    \
+  })
+
+#define __arch_atomic_increment_val_64(mem)                                                   \
+  (abort (), (__typeof (*mem)) 0)
+
+#define atomic_increment_val(mem)                                                             \
+  ({                                                                                          \
+    __typeof (*(mem)) __result;                                                               \
+    if (sizeof (*(mem)) == 4)                                                                 \
+      __result = __arch_atomic_increment_val_32 (mem);                                        \
+    else if (sizeof (*(mem)) == 8)                                                            \
+      __result = __arch_atomic_increment_val_64 (mem);                                        \
+    else                                                                                      \
+       abort ();                                                                              \
+    __result;                                                                                 \
+  })
+
+#define atomic_increment(mem) ({ atomic_increment_val (mem); (void) 0; })
+
+#define __arch_atomic_decrement_val_32(mem)                                                   \
+  ({                                                                                          \
+    __typeof (*(mem)) __val;                                                                  \
+    int test;                                                                                 \
+    __asm __volatile (                                                                        \
+                "   addc    r0, r0, r0;"    /* clean carry bit*/                              \
+                "1: lwx     %0, %3, r0;"    /* atomic_add_return*/                            \
+                "   addic   %1, r0, 0;"     /* check Carry*/                                  \
+                "   bnei    %1, 1b;"        /* jump if Carry is set*/                         \
+                "   rsubi   %0, %0, 1;"     /* sub 1 */                                       \
+                "   swx     %0, %3, r0;"    /* save value back */                             \
+                "   addic   %1, r0, 0;"     /* check Carry if saving was OK */                \
+                "   bnei    %1, 1b;"        /* jump if swx wasn't successful */               \
+                    : "=&r" (__val),        /* %0 */                                          \
+                    "=&r" (test),           /* %1 */                                          \
+                    "=m" (*mem)             /* Dummy output so gcc knows writing *mem */      \
+                    : "r" (mem),            /* %3 */                                          \
+                    "m" (*mem)              /* %4 */                                          \
+                    : "cc", "memory");                                                        \
+    __val;                                                                                    \
+  })
+
+#define __arch_atomic_decrement_val_64(mem)                                                   \
+  (abort (), (__typeof (*mem)) 0)
+
+#define atomic_decrement_val(mem)                                                             \
+  ({                                                                                          \
+    __typeof (*(mem)) __result;                                                               \
+    if (sizeof (*(mem)) == 4)                                                                 \
+      __result = __arch_atomic_decrement_val_32 (mem);                                        \
+    else if (sizeof (*(mem)) == 8)                                                            \
+      __result = __arch_atomic_decrement_val_64 (mem);                                        \
+    else                                                                                      \
+       abort ();                                                                              \
+    __result;                                                                                 \
+  })
+
+
+#define atomic_decrement(mem) ({ atomic_decrement_val (mem); (void) 0; })
-- 
1.7.1

